# 使用数据集指南

本文档介绍如何使用 SandboxFusion 的代码评估数据集功能。

---

## 支持的数据集总览

SandboxFusion 集成了 13+ 个标准代码评估数据集：

| 数据集 | 类型 | 说明 | 主要用途 |
|--------|------|------|---------|
| **HumanEval** | AutoEval | OpenAI 官方，164 道 Python 函数题 | 代码生成评估 |
| **MBPP** | AutoEval | Google 教科书式编程任务 | 基础编程能力 |
| **MultiPL-E** | AutoEval | HumanEval 多语言版本 | 多语言代码生成 |
| **MBXP** | AutoEval | MBPP 多语言版本 | 多语言基础编程 |
| **PAL-Math** | AutoEval | 数学推理（7 个子数据集） | 数学问题求解 |
| **Code Contests** | CommonOJ | 竞赛编程问题 | 算法竞赛能力 |
| **CRUXEval** | AutoEval | 代码理解与修复 | 代码审查能力 |
| **LiveCodeBench** | AutoEval | 持续更新的代码基准 | 实时能力评估 |
| **NaturalCodeBench** | AutoEval | 自然语言到代码（402 题） | 真实场景编程 |
| **FullStackBench** | AutoEval | Web 全栈开发 | Web 开发能力 |
| **Verilog-Eval** | AutoEval | 硬件描述语言 | 硬件设计能力 |
| **MiniF2F** | AutoEval | 形式化证明 | 定理证明能力 |
| **AiderBenchmark** | AutoEval | 代码编辑与修复 | 代码修复能力 |

---

## 数据集类型说明

### AutoEval 类型

基于测试代码进行评估，数据结构包含：

```python
{
    'id': '问题唯一标识',
    'content': '问题描述/提示',
    'labels': ['标签列表'],
    'test': '测试代码',
    'canonical_solution': '参考答案'
}
```

**适用场景**：函数实现、代码补全、单元测试驱动的评估

### CommonOJ 类型

基于标准输入/输出进行评估（类似 OJ 系统）：

```python
{
    'id': '问题唯一标识',
    'content': '问题描述',
    'test_cases': [
        {'input': '输入1', 'output': '期望输出1'},
        {'input': '输入2', 'output': '期望输出2'}
    ]
}
```

**适用场景**：竞赛编程、算法题

---

## 常用数据集详解

### HumanEval

OpenAI 发布的代码生成基准，包含 164 道 Python 函数编程题。

#### 获取提示

```python
from sandbox_fusion import get_prompts, GetPromptsRequest

prompts = get_prompts(GetPromptsRequest(
    dataset='humaneval',
    config={
        'language': 'python',
        'locale': 'en'  # 提示语言：en 或 zh
    }
))

for prompt in prompts[:3]:
    print(f"ID: {prompt.id}")
    print(f"提示: {prompt.prompt[:200]}...")
    print("---")
```

#### 提交评估

```python
from sandbox_fusion import submit, SubmitRequest, TestConfig

result = submit(SubmitRequest(
    dataset='humaneval',
    id='0',  # HumanEval/0
    completion='''
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    for i in range(len(numbers)):
        for j in range(i + 1, len(numbers)):
            if abs(numbers[i] - numbers[j]) < threshold:
                return True
    return False
''',
    config=TestConfig(
        language='python',
        run_timeout=20
    )
))

print(f"是否通过: {result.accepted}")
```

#### 配置选项

| 参数 | 说明 | 默认值 |
|------|------|--------|
| `is_freeform` | 指令微调模式（不提供函数签名） | `False` |
| `locale` | 提示语言 | `'en'` |
| `run_timeout` | 执行超时 | 20 秒 |

---

### MBPP (Mostly Basic Python Problems)

Google 发布的基础 Python 编程数据集，问题范围 ID 11-510。

#### 使用示例

```python
prompts = get_prompts(GetPromptsRequest(
    dataset='mbpp',
    config={
        'is_fewshot': True  # 是否添加示例
    }
))

result = submit(SubmitRequest(
    dataset='mbpp',
    id='11',  # 第一道题
    completion='''
def remove_Occ(s, ch):
    first = s.find(ch)
    last = s.rfind(ch)
    if first == -1:
        return s
    return s[:first] + s[first+1:last] + s[last+1:]
''',
    config=TestConfig(
        language='python',
        run_timeout=20
    )
))
```

#### 配置选项

| 参数 | 说明 | 默认值 |
|------|------|--------|
| `is_fewshot` | 是否添加示例 | `True` |
| `run_timeout` | 执行超时 | 20 秒 |

---

### PAL-Math (数学推理)

使用 Python 代码解决数学问题，包含 7 个子数据集：

| 子数据集 | 说明 |
|---------|------|
| GSM8k | 小学数学应用题 |
| MATH | 竞赛数学问题 |
| GSM-Hard | GSM8k 困难版本 |
| SVAMP | 简单变体数学问题 |
| TabMWP | 表格数学问题 |
| ASDiv | 多样化数学问题 |
| MAWPS | 数学应用文字问题 |

#### 使用示例

```python
# 获取 GSM8k 提示
prompts = get_prompts(GetPromptsRequest(
    dataset='palmath',
    config={
        'subset': 'gsm8k'
    }
))

# 提交评估（代码需要输出数字答案）
result = submit(SubmitRequest(
    dataset='palmath',
    id='0',
    completion='''
# Problem: Janet's ducks lay 16 eggs per day...
eggs_per_day = 16
eggs_for_breakfast = 3
eggs_for_baking = 4
eggs_sold = eggs_per_day - eggs_for_breakfast - eggs_for_baking
price_per_egg = 2
daily_income = eggs_sold * price_per_egg
print(daily_income)
''',
    config=TestConfig(
        language='python',
        compile_timeout=20,
        run_timeout=20
    )
))
```

---

### Code Contests (竞赛编程)

来自编程竞赛的算法问题，使用标准输入/输出格式（CommonOJ 类型）。

#### 使用示例

```python
prompts = get_prompts(GetPromptsRequest(
    dataset='codecontests',
    config={
        'language': 'cpp',
        'locale': 'en'
    }
))

# C++ 解答
result = submit(SubmitRequest(
    dataset='codecontests',
    id='0',
    completion='''
#include <iostream>
using namespace std;

int main() {
    int n;
    cin >> n;
    // 解题逻辑...
    cout << result << endl;
    return 0;
}
''',
    config=TestConfig(
        language='cpp',
        compile_timeout=20,
        run_timeout=20
    )
))
```

#### 配置选项

| 参数 | 说明 | 默认值 |
|------|------|--------|
| `language` | 编程语言 | `'cpp'` |
| `locale` | 题目语言 | `'en'` |
| `compile_timeout` | 编译超时 | 20 秒 |
| `run_timeout` | 运行超时 | 20 秒 |

---

### CRUXEval (代码理解)

Meta 发布的代码理解与修复数据集，有两种模式：

| 模式 | 说明 |
|------|------|
| `input` | 给定代码和输出，预测输入 |
| `output` | 给定代码和输入，预测输出 |

#### 使用示例

```python
prompts = get_prompts(GetPromptsRequest(
    dataset='cruxeval',
    config={
        'mode': 'output',  # 或 'input'
        'coding_wrap_prompt': True,  # 使用聊天格式包装
        'use_cot': False  # 添加思维链指导
    }
))

result = submit(SubmitRequest(
    dataset='cruxeval',
    id='0',
    completion='[output prediction]',
    config=TestConfig(
        language='python',
        run_timeout=20
    )
))
```

---

### NaturalCodeBench

更接近真实用户提示的代码生成数据集，402 道题（Python/Java × 中文/英文）。

#### 使用示例

```python
prompts = get_prompts(GetPromptsRequest(
    dataset='naturalcodebench',
    config={
        'language': 'python',
        'locale': 'zh'  # 中文提示
    }
))

result = submit(SubmitRequest(
    dataset='naturalcodebench',
    id='0',
    completion='def solution(): ...',
    config=TestConfig(
        language='python',
        compile_timeout=40,
        run_timeout=40
    )
))
```

---

### FullStackBench (全栈开发)

字节跳动发布的 Web 全栈开发评估数据集。

#### 使用示例

```python
prompts = get_prompts(GetPromptsRequest(
    dataset='fullstackbench',
    config={}
))

result = submit(SubmitRequest(
    dataset='fullstackbench',
    id='0',
    completion='// solution code',
    config=TestConfig(
        compile_timeout=50,
        run_timeout=50
    )
))
```

---

## 自定义数据提交

如果需要使用自定义数据（不在预设数据集中），可以使用 `provided_data` 参数：

### AutoEval 类型自定义数据

```python
custom_data = {
    'id': 'custom_001',
    'content': '实现一个函数，计算两个数的和',
    'test': '''
def test_add():
    assert add(1, 2) == 3
    assert add(-1, 1) == 0
    assert add(0, 0) == 0

test_add()
print("All tests passed!")
''',
    'canonical_solution': 'def add(a, b): return a + b'
}

result = submit(SubmitRequest(
    dataset='autoeval',  # 使用 autoeval 数据集类型
    id='0',  # 当 provided_data 存在时，id 会被忽略
    completion='''
def add(a, b):
    return a + b
''',
    config=TestConfig(
        language='python',
        dataset_type='AutoEval',
        provided_data=custom_data  # 自定义数据
    )
))
```

### CommonOJ 类型自定义数据

```python
custom_data = {
    'id': 'custom_002',
    'content': '读取两个整数，输出它们的和',
    'test_cases': [
        {'input': '1 2\n', 'output': '3\n'},
        {'input': '10 20\n', 'output': '30\n'},
        {'input': '-5 5\n', 'output': '0\n'}
    ]
}

result = submit(SubmitRequest(
    dataset='common_oj',
    id='0',
    completion='''
a, b = map(int, input().split())
print(a + b)
''',
    config=TestConfig(
        language='python',
        dataset_type='CommonOJ',
        provided_data=custom_data
    )
))
```

**注意**：当 `provided_data` 存在时，`id` 参数会被忽略，但 `dataset` 参数仍需保持，因为某些实现依赖它选择语言或处理逻辑。

---

## 评估结果解析

### EvalResult 结构

```python
class EvalResult:
    accepted: bool          # 是否通过
    extracted_code: str     # 提取的代码
    tests: List[EvalTestCase]  # 测试用例结果
    extra: Dict             # 额外信息
```

### 详细结果分析

```python
result = submit(request)

print(f"总体结果: {'通过' if result.accepted else '失败'}")
print(f"提取的代码:\n{result.extracted_code}")

# 分析每个测试用例
if result.tests:
    for i, test in enumerate(result.tests):
        print(f"\n测试 {i + 1}:")
        print(f"  状态: {test.status}")
        print(f"  输出: {test.stdout[:100] if test.stdout else 'N/A'}...")
        if test.stderr:
            print(f"  错误: {test.stderr[:100]}...")

# 额外信息
if result.extra:
    print(f"\n额外信息: {result.extra}")
```

---

## 批量评估示例

```python
from sandbox_fusion import (
    get_prompts, submit, run_concurrent,
    GetPromptsRequest, SubmitRequest, TestConfig
)

# 1. 获取所有提示
prompts = get_prompts(GetPromptsRequest(
    dataset='humaneval',
    config={'language': 'python'}
))

# 2. 模拟 LLM 生成代码（实际场景替换为真实 LLM）
def generate_completion(prompt):
    # 这里应该调用你的 LLM
    return "def solution(x): pass"

# 3. 准备所有请求
requests = []
for prompt in prompts:
    completion = generate_completion(prompt.prompt)
    requests.append(SubmitRequest(
        dataset='humaneval',
        id=prompt.id,
        completion=completion,
        config=TestConfig(language='python')
    ))

# 4. 并发提交评估
results = run_concurrent(
    func=submit,
    args_list=requests,
    max_workers=10
)

# 5. 统计结果
passed = sum(1 for r in results if r.accepted)
total = len(results)
print(f"通过率: {passed}/{total} = {passed/total:.2%}")
```

---

## 数据集选择指南

| 评估目标 | 推荐数据集 |
|---------|----------|
| 基础代码生成能力 | HumanEval, MBPP |
| 多语言能力 | MultiPL-E, MBXP |
| 算法与数据结构 | Code Contests |
| 数学推理 | PAL-Math (GSM8k, MATH) |
| 代码理解 | CRUXEval |
| 真实场景编程 | NaturalCodeBench |
| Web 开发 | FullStackBench |
| 硬件设计 | Verilog-Eval |
| 形式化证明 | MiniF2F |

---

## 下一步

- [05-API参考.md](05-API参考.md) - 查看完整的数据模型和 API 参考
